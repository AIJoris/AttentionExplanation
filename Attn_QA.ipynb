{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UtilsQA import *\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetQA import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_CNN()\n",
    "model = load_model(dataset)\n",
    "test_data = dataset.test_data\n",
    "test_data.predict, test_data.attn_hat = evaluate_and_print(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_graphs(dataset, model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datagen in tqdm_notebook(datagens) :\n",
    "    dataset = datagen()\n",
    "    print(dataset.name)\n",
    "#     dataset.display_stats()\n",
    "    model = load_model(dataset)\n",
    "    test_data = dataset.test_data\n",
    "    test_data.predict, test_data.attn_hat = evaluate_and_print(model, test_data)\n",
    "    generate_graphs(dataset, model, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING**\n",
    "============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = get_CNN()\n",
    "# train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = get_SNLI()\n",
    "# train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = get_Babi_1()\n",
    "# train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = get_Babi_2()\n",
    "# train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = get_Babi_3()\n",
    "# train(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "print_example(dataset.vec, test_data, test_data.predict, test_data.attn_hat, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi Adversarial Attention**\n",
    "==============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_adversarial_outputs = model.adversarial_multi(test_data.P, test_data.Q, test_data.E)\n",
    "pdump(model, multi_adversarial_outputs, 'multi_adversarial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_adversarial_outputs = pload(model, 'multi_adversarial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emax_jds, emax_adv_attn, emax_ad_y = plot_multi_adversarial(test_data, test_data.predict, test_data.attn_hat, \n",
    "                                                            multi_adversarial_outputs, \n",
    "                                                            epsilon=0.03,\n",
    "                                                            by_class=False, dirname=model.dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(emax_jds)[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 860\n",
    "print(\" \".join(vec.map2words(test_data.Q[n])))\n",
    "print(vec.idx2entity[test_data.A[n]], vec.idx2entity[predict[n]], vec.idx2entity[emax_ad_y[n]])\n",
    "print_adversarial_example(vec.map2words(test_data.P[n]), attn_hat[n], emax_adv_attn[n])\n",
    "# for k in range(5) :\n",
    "#     print_attn(vec.map2words(test_data.P[n]), ad_attn[n][k])\n",
    "#     print(vec.idx2entity[ad_y[n][k]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Permutations**\n",
    "================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_outputs = model.permute_attn(test_data.P, test_data.Q, test_data.E)\n",
    "pdump(model, perm_outputs, 'permutations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_permutations(perm_outputs, test_data, test_data.predict, test_data.attn_hat, dirname=model.dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zero Runs**\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datagen in tqdm_notebook(datagens) :\n",
    "    dataset = datagen()\n",
    "    print(dataset.name)\n",
    "    model = load_model(dataset)\n",
    "    test_data = dataset.test_data\n",
    "    test_data.predict, test_data.attn_hat = evaluate_and_print(model, test_data)\n",
    "    zero_outputs = model.zero_H_run(test_data.P, test_data.Q, test_data.E)\n",
    "    pdump(model, zero_outputs, 'zero_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove and Runs**\n",
    "==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datagen in tqdm_notebook(datagens) :\n",
    "    dataset = datagen()\n",
    "    print(dataset.name)\n",
    "    model = load_model(dataset)\n",
    "    test_data = dataset.test_data\n",
    "    test_data.predict, test_data.attn_hat = evaluate_and_print(model, test_data)\n",
    "    remove_odiffs = model.remove_and_run(test_data.P, test_data.Q, test_data.E)\n",
    "    pdump(model, remove_odiffs, 'remove_and_run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sampling Outputs**\n",
    "===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datagen in tqdm_notebook(datagens) :\n",
    "    dataset = datagen()\n",
    "    print(dataset.name)\n",
    "    model = load_model(dataset)\n",
    "    test_data = dataset.test_data\n",
    "    test_data.predict, test_data.attn_hat = evaluate_and_print(model, test_data)\n",
    "\n",
    "    model.vec = dataset.vec\n",
    "    outputs = model.sampling_top(test_data.P, test_data.Q, test_data.E)\n",
    "    pdump(model, outputs, 'sampled_outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
