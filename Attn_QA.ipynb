{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UtilsQA import *\n",
    "from DatasetQA import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datagen in [get_CNN] :\n",
    "    dataset = datagen()\n",
    "    print(dataset.name)\n",
    "    attentions = ['dot']\n",
    "    for wd in attentions :\n",
    "        dataset.attention = wd\n",
    "        train(dataset, name='attn='+wd, exp_dirname='exp_attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datagen in tqdm_notebook(datagens) :\n",
    "    dataset = datagen()\n",
    "    print(dataset.name)\n",
    "    model = load_model(dataset)\n",
    "    test_data = dataset.test_data\n",
    "    test_data.predict, test_data.attn_hat = evaluate_and_print(model, test_data)\n",
    "    \n",
    "#     multi_adversarial_outputs = pload(model, 'multi_adversarial')\n",
    "#     emax_jds, emax_adv_attn, emax_ad_y, emax_diffs = plot_multi_adversarial(test_data, test_data.predict, test_data.attn_hat, \n",
    "#                                                             multi_adversarial_outputs, \n",
    "#                                                             epsilon=0.03,\n",
    "#                                                             by_class=False, dirname=model.dirname)\n",
    "#     print_adversarial_examples(dataset, test_data, test_data.predict, test_data.attn_hat, \n",
    "#                                emax_jds, emax_adv_attn, emax_ad_y, emax_diffs, dirname=model.dirname)\n",
    "    \n",
    "#     generate_graphs(dataset, model, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING**\n",
    "============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = get_CNN()\n",
    "# train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = get_SNLI()\n",
    "# train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = get_Babi_1()\n",
    "# train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = get_Babi_2()\n",
    "# train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = get_Babi_3()\n",
    "# train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datagen in tqdm_notebook(datagens) :\n",
    "    dataset = datagen()\n",
    "    print(dataset.name)\n",
    "    models = load_model_from_exp(dataset, exp_dirname='exp_attention')\n",
    "    for k, model in models.items() :\n",
    "        if 'dot' in k :\n",
    "            display(HTML('<h2>'+k+'</h2>'))\n",
    "            test_data = dataset.test_data\n",
    "            test_data.predict, test_data.attn_hat = evaluate_and_print(model, test_data)\n",
    "            dataset.name += 'dot'\n",
    "            generate_graphs(dataset, model, test_data)\n",
    "#             remove_odiffs = model.remove_and_run(test_data.P, test_data.Q, test_data.E)\n",
    "#             pdump(model, remove_odiffs, 'remove_and_run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "print_example(dataset.vec, test_data, test_data.predict, test_data.attn_hat, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi Adversarial Attention**\n",
    "==============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_adversarial_outputs = model.adversarial_multi(test_data.P, test_data.Q, test_data.E)\n",
    "pdump(model, multi_adversarial_outputs, 'multi_adversarial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_adversarial_outputs = pload(model, 'multi_adversarial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emax_jds, emax_adv_attn, emax_ad_y, emax_diffs = plot_multi_adversarial(test_data, test_data.predict, test_data.attn_hat, \n",
    "                                                            multi_adversarial_outputs, \n",
    "                                                            epsilon=0.03,\n",
    "                                                            by_class=False, dirname=model.dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_adversarial_examples(dataset, test_data, test_data.predict, test_data.attn_hat, \n",
    "                           emax_jds, emax_adv_attn, emax_ad_y, emax_diffs, dirname=model.dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 860\n",
    "print(\" \".join(vec.map2words(test_data.Q[n])))\n",
    "print(vec.idx2entity[test_data.A[n]], vec.idx2entity[predict[n]], vec.idx2entity[emax_ad_y[n]])\n",
    "print_adversarial_example(vec.map2words(test_data.P[n]), attn_hat[n], emax_adv_attn[n])\n",
    "# for k in range(5) :\n",
    "#     print_attn(vec.map2words(test_data.P[n]), ad_attn[n][k])\n",
    "#     print(vec.idx2entity[ad_y[n][k]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Permutations**\n",
    "================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_outputs = model.permute_attn(test_data.P, test_data.Q, test_data.E)\n",
    "# pdump(model, perm_outputs, 'permutations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_permutations(perm_outputs, test_data, test_data.predict, test_data.attn_hat, dirname=model.dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove and Runs**\n",
    "==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datagen in tqdm_notebook(datagens) :\n",
    "    dataset = datagen()\n",
    "    print(dataset.name)\n",
    "    model = load_model(dataset)\n",
    "    test_data = dataset.test_data\n",
    "    test_data.predict, test_data.attn_hat = evaluate_and_print(model, test_data)\n",
    "    remove_odiffs = model.remove_and_run(test_data.P, test_data.Q, test_data.E)\n",
    "    pdump(model, remove_odiffs, 'remove_and_run')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
