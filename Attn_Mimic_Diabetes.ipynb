{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from Utils import exec_jupyter; exec_jupyter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "from Utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'preprocess/')\n",
    "import vectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = pickle.load(open('preprocess/MIMIC/vec_icd9.p', 'rb'))\n",
    "vec.output_size = len(vec.label2idx)\n",
    "add_frequencies(vec, vec.seqs['train']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.Attn_Word_Pert as M\n",
    "Model = M.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_label = vec.label2idx['250.00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Xt = vec.seqs['train'], vec.seqs['test']\n",
    "y, yt = vec.label_one_hot['train'][:, diabetes_label], vec.label_one_hot['test'][:, diabetes_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_t = np.argsort([len(x) for x in Xt])\n",
    "Xt = [Xt[i] for i in len_t]\n",
    "yt = [yt[i] for i in len_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weight = 1 #len(y)/sum(y) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "def train(name='') :\n",
    "    model = Model(vec.vocab_size, vec.word_dim, 32, dirname='diab', hidden_size=128, pre_embed=vec.embeddings, pos_weight=pos_weight)\n",
    "    best_f1 = 0.0\n",
    "    for i in tqdm_notebook(range(10)) :\n",
    "        loss = model.train(X, y)\n",
    "        o, he = model.evaluate(Xt)\n",
    "        o = np.array(o)\n",
    "        rep = classification_report(yt, (o > 0.5))\n",
    "        f1 = f1_score(yt, (o > 0.5), pos_label=1)\n",
    "        print(rep)\n",
    "        stmt = '%s, %s' % (i, loss)\n",
    "        if f1 > best_f1 :\n",
    "            best_f1 = f1\n",
    "            dirname = model.save_values(add_name=name, save_model=True)\n",
    "            print(\"Model Saved\", f1)\n",
    "        else :\n",
    "            dirname = model.save_values(add_name=name, save_model=False)\n",
    "            print(\"Model not saved\", f1)\n",
    "        f = open(dirname + '/epoch.txt', 'a')\n",
    "        f.write(stmt + '\\n')\n",
    "        f.write(rep + '\\n')\n",
    "        f.close()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(name='TEST_icd9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(dirname) :\n",
    "    model = Model(vec.vocab_size, vec.word_dim, 32, dirname='mimic', hidden_size=128, pre_embed=vec.embeddings)\n",
    "    model.dirname = dirname\n",
    "    model.load_values(dirname)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('outputs/attn_word_diab/SunOct1412:56:302018_TEST_icd9/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_hat, attn_hat = evaluate_and_print(model, Xt, yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_entropy(Xt, attn_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __SAMPLING__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vec = vec\n",
    "sampled_output = model.sampling_top(Xt, sample_vocab=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(sampled_output, open(model.dirname + '/sampled.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_output = pickle.load(open(model.dirname + '/sampled.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_medians_from_sampling_top(sampled_output, attn_hat, yt_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distractors = get_distractors(sampled_output, attn_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRADIENTS**\n",
    "========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = model.gradient_mem(Xt)\n",
    "process_grads(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grads(Xt, attn_hat, grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Permutation**\n",
    "==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perms = model.permute_attn(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_permutations(perms, Xt, yt_hat, attn_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adversarial**\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_outputs = model.adversarial(Xt, _type='uniform')\n",
    "ad_y, ad_attn = adversarial_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jds = plot_adversarial(Xt, yt_hat, attn_hat, adversarial_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = list(np.where(np.logical_and(np.array(jds) > 0.3, yt_hat > 0.8))[0])[:30]\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 731\n",
    "print_adversarial_example(vec.map2words(X[n]), attn_hat[n], ad_attn[n])\n",
    "print(yt_hat[n], ad_y[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copy Runs Previous and Subsequent**\n",
    "====================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_runs = model.copy_H_run(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_outputs, copy_attn, copy_H_diff = copy_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_diff(Xt, attn_hat, copy_attn, \"Attention vs Copy Attention\", usehexbin=True)\n",
    "plot_y_diff(X, attn_hat, yt_hat, copy_outputs, usehexbin=True)\n",
    "plot_attn_diff(Xt, attn_hat, copy_H_diff, \"Attention vs H difference\", usehexbin=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zero Runs**\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_runs = model.zero_H_run(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_outputs, zero_H_diff = zero_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_diff(Xt, attn_hat, zero_H_diff, title=\"Attention vs zero H diff\", usehexbin=True)\n",
    "plot_y_diff(X, attn_hat, yt_hat, zero_outputs, usehexbin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_outputs = model.remove_and_run(Xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pertubation Embeddings**\n",
    "=========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturb_outputs = model.perturbation_embedding(Xt, num_pert=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
