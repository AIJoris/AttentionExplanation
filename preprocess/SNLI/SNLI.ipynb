{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-15 17:03:02--  https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 94550081 (90M) [application/zip]\n",
      "Saving to: ‘snli_1.0.zip’\n",
      "\n",
      "snli_1.0.zip        100%[===================>]  90.17M  25.0MB/s    in 5.0s    \n",
      "\n",
      "2019-04-15 17:03:08 (18.0 MB/s) - ‘snli_1.0.zip’ saved [94550081/94550081]\n",
      "\n",
      "Archive:  snli_1.0.zip\n",
      "   creating: snli_1.0/\n",
      "  inflating: snli_1.0/.DS_Store      \n",
      "   creating: __MACOSX/\n",
      "   creating: __MACOSX/snli_1.0/\n",
      "  inflating: __MACOSX/snli_1.0/._.DS_Store  \n",
      " extracting: snli_1.0/Icon           \n",
      "  inflating: __MACOSX/snli_1.0/._Icon  \n",
      "  inflating: snli_1.0/README.txt     \n",
      "  inflating: __MACOSX/snli_1.0/._README.txt  \n",
      "  inflating: snli_1.0/snli_1.0_dev.jsonl  \n",
      "  inflating: snli_1.0/snli_1.0_dev.txt  \n",
      "  inflating: snli_1.0/snli_1.0_test.jsonl  \n",
      "  inflating: snli_1.0/snli_1.0_test.txt  \n",
      "  inflating: snli_1.0/snli_1.0_train.jsonl  \n",
      "  inflating: snli_1.0/snli_1.0_train.txt  \n",
      "  inflating: __MACOSX/._snli_1.0     \n"
     ]
    }
   ],
   "source": [
    "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
    "!unzip snli_1.0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['train', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {k:[] for k in keys}\n",
    "for k in keys :\n",
    "    for line in open('snli_1.0/snli_1.0_' + k + '.jsonl').readlines() :\n",
    "        data[k].append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorizer import cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, q, a = {}, {}, {}\n",
    "\n",
    "for k in keys :\n",
    "    p[k] = [cleaner(x['sentence1']) for x in data[k] if x['gold_label'] != '-']\n",
    "    q[k] = [cleaner(x['sentence2']) for x in data[k] if x['gold_label'] != '-']\n",
    "    a[k] = [cleaner(x['gold_label']) for x in data[k] if x['gold_label'] != '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = 3\n",
    "#embedding : glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_list = ['neutral', 'contradiction', 'entailment']\n",
    "f = open('entity_list.txt', 'w')\n",
    "f.write(\"\\n\".join(entity_list))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_paragraphs = []\n",
    "df_questions = []\n",
    "df_answers = []\n",
    "df_exp_splits = []\n",
    "\n",
    "for k in keys :\n",
    "    df_paragraphs += p[k]\n",
    "    df_questions += q[k]\n",
    "    df_answers += a[k]\n",
    "    df_exp_splits += [k] * len(p[k])\n",
    "    \n",
    "df = {'paragraph' : df_paragraphs, 'question' : df_questions, 'answer' : df_answers, 'exp_split' : df_exp_splits}\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('snli_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size :  20981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../.vector_cache/glove.840B.300d.zip: 2.18GB [01:25, 25.5MB/s]                            \n",
      "100%|█████████▉| 2195278/2196017 [03:05<00:00, 11878.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20339 words in model out of 20981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 2195278/2196017 [03:20<00:00, 11878.46it/s]"
     ]
    }
   ],
   "source": [
    "%run \"../preprocess_data_QA.py\" --data_file snli_dataset.csv --output_file ./vec_snli.p --all_answers_file entity_list.txt \\\n",
    "--word_vectors_type glove.840B.300d --min_df 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
